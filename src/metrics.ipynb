{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Reading Tools\n",
    "for paper \"Transformer-Powered Content-Aware Collaborative Filtering with Cross-System Contrastive Learning\"\n",
    "\n",
    "Author: Weizhe Lin\n",
    "\n",
    "Created: 10/10/2021 for Github Release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from easydict import EasyDict\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing import event_accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        return None\n",
    "    with open(file_path, 'r') as f:\n",
    "        temp = json.loads(f.read())\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tb(tb_file):\n",
    "    ea = event_accumulator.EventAccumulator(tb_file,\n",
    "    size_guidance={ # see below regarding this argument\n",
    "        event_accumulator.COMPRESSED_HISTOGRAMS: 500,\n",
    "        event_accumulator.IMAGES: 4,\n",
    "        event_accumulator.AUDIO: 4,\n",
    "        event_accumulator.SCALARS: 0,\n",
    "        event_accumulator.HISTOGRAMS: 1,\n",
    "    })\n",
    "    ea.Reload()\n",
    "    return ea\n",
    "def find_best_performance(ea, to_find='test/recall_at_20'):\n",
    "    metrics_data = pd.DataFrame(ea.Scalars(to_find))\n",
    "    best_epoch = metrics_data['step'][metrics_data['value'].idxmax(axis=-1)]\n",
    "    best_performance = {}\n",
    "    for metric in ea.scalars.Keys():\n",
    "        cur_metrics_data = pd.DataFrame(ea.Scalars(metric))\n",
    "        cur_metrics_data = cur_metrics_data[cur_metrics_data['step']==best_epoch]\n",
    "#         print(cur_metrics_data.iloc[0])\n",
    "        best_performance.setdefault(metric, {\n",
    "            'best':cur_metrics_data.iloc[0]['value'],\n",
    "            'epoch': best_epoch\n",
    "        })\n",
    "#     print(best_performance)\n",
    "    return best_performance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all experiment folders\n",
    "EXPERIMENT_FOLDERS = [\n",
    "    '/path/to/project/Experiments',\n",
    "]\n",
    "# all tensorboard folders\n",
    "TB_FOLDERS = [\n",
    "    '/path/to/project/Data_TB/tb_logs',\n",
    "]\n",
    "# which metric is considered the best epoch\n",
    "READ_METRICS_AT_BEST = 'test/recall_at_100'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {}\n",
    "for exp_folder in EXPERIMENT_FOLDERS:\n",
    "    if os.path.exists(exp_folder):\n",
    "        for exp_name in tqdm(os.listdir(exp_folder)):\n",
    "            experiments.setdefault(exp_name, {})\n",
    "            exp_path = os.path.join(exp_folder, exp_name)\n",
    "            config_path = os.path.join(exp_path, 'train', 'config.jsonnet')\n",
    "            metrics_path = os.path.join(exp_path, 'train', 'saved_model', 'metrics.json')\n",
    "            tb_path = ''\n",
    "            for tb_folder in TB_FOLDERS:\n",
    "                test_tb_path = os.path.join(tb_folder, exp_name)\n",
    "                if os.path.exists(test_tb_path):\n",
    "                    tb_path = test_tb_path\n",
    "                    break\n",
    "            best_perf = None\n",
    "            if tb_path:\n",
    "                tb_files = os.listdir(tb_path)\n",
    "                #print('found tb_path', tb_files)\n",
    "                for tb_file in tb_files:\n",
    "                    try:\n",
    "                        ea = read_tb(os.path.join(tb_path, tb_file))\n",
    "                        best_perf = find_best_performance(ea, READ_METRICS_AT_BEST)\n",
    "                        break\n",
    "                    except:\n",
    "                        print('reading failed at', os.path.join(tb_path, tb_file))\n",
    "                        pass\n",
    "                    \n",
    "            metrics = read_json(metrics_path)\n",
    "            if metrics is not None and best_perf is not None:\n",
    "                metrics.update(best_perf)\n",
    "            print(exp_name)\n",
    "#             print(metrics)\n",
    "            experiments[exp_name] = {\n",
    "                'exp_name': exp_name,\n",
    "                'path': exp_path,\n",
    "                'metrics': metrics,\n",
    "            }\n",
    "    else:\n",
    "        print('this path not exist')\n",
    "experiments = EasyDict(experiments)\n",
    "# pprint(experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out invalid experiments\n",
    "filtered_experiments = {exp_name:exp_data for exp_name, exp_data in experiments.items() if exp_data.metrics is not None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_data_dict = {}\n",
    "\n",
    "for exp_id, exp_name in enumerate(filtered_experiments.keys()):\n",
    "    exp_data_dict.setdefault(exp_id, {})\n",
    "    gather = {}\n",
    "    gather['exp_name'] = exp_name\n",
    "    for metrics_name, metrics_data in filtered_experiments[exp_name].metrics.items():\n",
    "        gather[metrics_name] = metrics_data.best\n",
    "    exp_data_dict[exp_id] = gather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_pd_data = pd.DataFrame.from_dict(exp_data_dict).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_experiments = [\n",
    "    'NRMS_BERT_Amazon_LR_0.0001_Layer_3_History_10_NoAttMask',\n",
    "    #...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie Dataset\n",
    "## 19days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_experiments = [\n",
    "    \"NRMS_BERT_Movie_19d_ColdStart_LR_0.0005_Layer_1_History_30_NoAttMask\",\n",
    "    #...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 39 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_experiments = [\n",
    "    #...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_df = exp_pd_data.loc[exp_pd_data['exp_name'].isin(select_experiments)].sort_values(by='test/recall_at_20', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_df[['exp_name', 'recall_at_20', 'recall_at_60', 'recall_at_100', 'ndcg_at_20', 'ndcg_at_60', 'ndcg_at_100', 'hit_ratio_at_20', 'hit_ratio_at_60', 'hit_ratio_at_100']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_df[['exp_name', 'test/recall_at_20', 'test/recall_at_60', 'test/recall_at_100', 'test/ndcg_at_20', 'test/ndcg_at_60', 'test/ndcg_at_100', 'test/hit_ratio_at_20', 'test/hit_ratio_at_60', 'test/hit_ratio_at_100']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_df[['exp_name', 'recall_at_20', 'cold_start_recall_at_20', 'ndcg_at_20', 'cold_start_ndcg_at_20']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
